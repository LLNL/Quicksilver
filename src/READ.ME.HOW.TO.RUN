
Last Updated 2016-06-07 

Initial Release Instructions for running.

-------------------------------------------------------------------------------
Running The Default Problem 

There is a default problem built into the executable, which will run if
no input is specified.  This may be run sequentially, with MPI, with threads,
or hybrid MPI + Threads.

One can adjust the default problem by specifying command line arguments.  To 
see a list of arguments, run qs --help, which will give a short one line 
description for each argument.  More complete documentation will be provided
in a later release of this application.

As an example, one may  increase the number of particles, and hence the 
amount of work to be done, by specifying the --nParticles option.  The 
following is an example of an MPI run with 4 mpi processes and 1,000,000
particles. 

   srun -n4 ./qs --nParticles=1000000


-------------------------------------------------------------------------------
Running From An Input Deck:

The Quicksilver development team is generating standard test cases as input
files.  These reside in the 'Input' subdirectory.  

A useful one to start with is homogeneousProblem_v3.inp

While the default test case has a particle source in a specific material and
location with the mesth, this homogeneous test problem is simplified and has 
sourcing throughout one material across the entire mesh.  This helps to
create a problem that is more load balanced from the start of the run.

Both of these problems are of interest (and others), and suit different 
studies.

Note, the specific homogeneousProblem_v3.inp test problem is designed to
use MPI_THREAD_MULTIPLE, and requires this support.  If the MPI library
does not support this, it may be disabled by setting the mpiThreadMultiple 
option within the deck to 0.[ mpiThreadMultiple has ben disable in QS ]

Also, that particular test problem requires the user to specify the 
number of I, J, and K ranks such that their product equals the number
of MPI processes requested: Thus a 4 mpi process run of this input deck
would look like:

   srun -n4 ./qs -i Input/homogeneousProblem_v3.inp -I 2 -J 2 -K 1

And an 8 MPI process run could look like:

   srun -n8 ./qs -i Input/homogeneousProblem_v3.inp -I 2 -J 2 -K 2


-------------------------------------------------------------------------------
A note on running with threads: 

If the code is compiled with threads, the code will run with threads in 
addition to MPI.  The OMP_NUM_THREADS env var will be used to set the
number of threads.  If one wishes to run without threads one may either
compile the code without threads ore set OMP_NUM_THREADS to 1.

-------------------------------------------------------------------------------
A note on the output generated:

The Preamble:

At the start of the run, the code generates a preamble of how it is running.
It looks simlar to

   MPI Initialized         : MPI_THREAD_MULTIPLE
   Copyright (c) 2016
   Lawrence Livermore National Security, LLC
   All Rights Reserved
   Quicksilver Version     : 2016-Jun-3-14:30:10
   Quicksilver Git Hash    : 6e6c03436f491c760d173c5c5fda681589f22ec4
   MPI Version             : 3.0
   Number of MPI ranks     : 8
   Number of OpenMP Threads: 4
   Number of OpenMP CPUs   : 8

This is useful information, and includes information abot the MPI Thread
run mode, the number of MPI processes, the number of threads per MPI process,
the number of available 'cores' OpenMP believes are available, and 
the specific Quicksilver version that is running.

The Problem Definition:

After the preamble, the code outputs the problem definition.  This is
quite useful, as a feature of Quicksilver, is that this section may
be placed into a file and then used as input on subsequent runs.  This
is how we created the Input test decks discussed above.

The simulation section is composed of the following sections which may
be placed into a file: Simulation, Geometry, Material, CrossSection.
There may be more than one of each section, if for instance there are
multiple materials or geometries in the problem.


The Run Output

As the code runs, it prints output each run step, these are various tallies,
such as  start number of particles, absorbed number of particles, and more. 
It also includes time spent in the three main phases of each run step: cycle
initialize, cycle tracking, and cycle finalize.

It can be useful to plot these values using a spread sheet to compare various
run modes for correctness and for relative performance.

There is also, at the end of the run, a coarse breakdown of time spent overall
in the above mentioned three code phases, as well as a few other sub timings
from cycle tracking.

-------------------------------------------------------------------------------
A note on asserts:

Asserts are used to stop the code when it is run incorrectly, the input is 
invalid, or an error is detected.  Read the assert message to see if it may 
be remedied quickly.  As a for instance, if the above mentioned test problem
homogeneousProblem_v3.inp is run with a code that was not compiled with
OpenMP thread, 

  qs: utilsMpi.cc:35: void mpiInit(int*, char***, Parameters*): 
        Assertion `false' failed.
  User requested mpiThreadMultiple support in a non-threaded code build

This is because that specific test problem attempts to initialize the
MPI library with support for MPI_THREAD_MULTIPLE, which requires a
threaded executable, and an MPI library which supports this feature.

--------------------------------------------------------------------------------
